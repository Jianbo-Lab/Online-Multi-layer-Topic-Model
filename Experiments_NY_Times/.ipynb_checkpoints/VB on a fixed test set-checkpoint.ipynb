{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The code below maps the whole dictionary to our selected vocab.\n",
    "import re\n",
    "import gzip\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "vocab_ny = file('dataset/vocab.nytimes.txt').readlines()\n",
    "vocab = file('./dictnostops.txt').readlines()\n",
    "revised_vocab = dict()\n",
    "for word in vocab:\n",
    "    word = word.lower()\n",
    "    word = re.sub(r'[^a-z]', '', word)\n",
    "    revised_vocab[word] = len(revised_vocab)\n",
    "    \n",
    "vocab = revised_vocab\n",
    " \n",
    "import numpy as np\n",
    "# phi is a map from the whole vocab to our selected vocab.\n",
    "phi = np.tile(0,len(vocab_ny))\n",
    "for j in range(len(vocab_ny)):\n",
    "    word = vocab_ny[j].lower() \n",
    "    word = re.sub(r'[^a-z]', '', word)\n",
    "    if word in vocab:\n",
    "        phi[j] = vocab[word]\n",
    "    else:\n",
    "        phi[j] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The code below converts the data into our required format.\n",
    "count_old = 0\n",
    "MM = 4686\n",
    "with gzip.open('docword.nytimes.txt.gz','r') as fin:\n",
    "    itr = 1\n",
    "    for line in fin:\n",
    "        triple = line.split()\n",
    "        if len(triple) == 3:\n",
    "            if int(triple[0]) % 64 == 1 and count_old != int(triple[0]):\n",
    "                count_old = int(triple[0])\n",
    "                # Write the previous two lists to the files.\n",
    "                if itr > 1:\n",
    "                    with open('dataset/wordids-%d.p'%(itr-1), 'wb') as f:\n",
    "                        cPickle.dump(wordids, f)\n",
    "                    with open('dataset/wordcts-%d.p'%(itr-1), 'wb') as f:\n",
    "                        cPickle.dump(wordcts, f)\n",
    "                    if itr == MM:\n",
    "                        break\n",
    "                # Create two new lists.\n",
    "                wordids = [[] for i in range(64)]\n",
    "                wordcts = [[] for i in range(64)]\n",
    "                itr = itr + 1\n",
    "            \n",
    "            if phi[int(triple[1]) - 1] != -1:\n",
    "                wordids[int(triple[0]) % 64 - 1] = wordids[int(triple[0]) % 64 - 1] + [phi[int(triple[1]) - 1]]\n",
    "                wordcts[int(triple[0]) % 64 - 1] = wordcts[int(triple[0]) % 64 - 1] + [int(triple[2])]\n",
    "            \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct the test dataset by combining the last few files after conversion.\n",
    "import re\n",
    "import gzip\n",
    "import numpy as np\n",
    "from six.moves import cPickle\n",
    "size = 4\n",
    "MM = 4686\n",
    "wordids = []\n",
    "wordcts = []\n",
    "for itr in range(MM - size,MM):\n",
    "    with open('dataset/wordids-%d.p'%itr, 'rb') as f:\n",
    "        wordids = wordids + cPickle.load(f)\n",
    "    with open('dataset/wordcts-%d.p'%itr, 'rb') as f:\n",
    "        wordcts = wordcts + cPickle.load(f)\n",
    "\n",
    "with open('dataset/wordids-test-small.p', 'wb') as f:\n",
    "    cPickle.dump(wordids, f)\n",
    "with open('dataset/wordcts-test-small.p', 'wb') as f:\n",
    "    cPickle.dump(wordcts, f)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test for the main program.\n",
    "import cPickle, string, numpy, getopt, sys, random, time, re, pprint\n",
    "\n",
    "def f():\n",
    "    \"\"\"\n",
    "    This function fits Wikipedia articles to the deep LDA model in an online style.\n",
    "    \"\"\"\n",
    "    perplexity_LDA = open(\"perplexity_LDA.txt\", \"w\")\n",
    "    perplexity_deep_LDA = open(\"perplexity_deep_LDA.txt\", \"w\")   \n",
    "    # The number of documents to analyze in each iteration\n",
    "    batchsize = 32\n",
    "    # The estimated total number of documents  \n",
    "    D = 5.13e6\n",
    "    # The number of topics\n",
    "    K1 = 50\n",
    "    K2 = 10\n",
    "    eta0 = 1 / np.float(K1)\n",
    "    eta1 = 1 / np.float(K2)\n",
    "    eta2 = 1 / np.float(K2) \n",
    "\n",
    "    # The total number of iterations\n",
    "    M = 10\n",
    " \n",
    "    vocab = file('dataset/vocab.nytimes.txt').readlines()\n",
    "    W = len(vocab)\n",
    "\n",
    "    # Initialize the algorithm.\n",
    "    model = OnlineLDA(vocab, K1, K2, D, eta0, eta1, eta2, 0, 1)\n",
    "    model_LDA = OnlineLDA(vocab, 100, 1, D, eta0, eta1, eta2, 1024, 0.7)\n",
    "    for iteration in range(0, M):\n",
    "        # Compute the held-out perplexity and fit them to the deep LDA model.\n",
    "        with open('dataset/wordids-%d.p'%(iteration+1), 'rb') as f:\n",
    "            wordids = cPickle.load(f)\n",
    "        with open('dataset/wordcts-%d.p'%(iteration+1), 'rb') as f:\n",
    "            wordcts = cPickle.load(f)\n",
    "         \n",
    "        model.update_lambda_docs(wordids,wordcts)\n",
    " \n",
    "        model_LDA.update_lambda_docs(wordids,wordcts)\n",
    "        if (iteration <= 64 or iteration % 15 == 0):\n",
    "            bound = model.perplexity_for_test()\n",
    "            bound_LDA = model_LDA.perplexity_for_test()\n",
    "            print '%d: held-out perplexity estimate = %f' % \\\n",
    "                (iteration, bound)\n",
    "            perplexity_deep_LDA.write(str(bound) + '\\n')   \n",
    "            print '%d: held-out perplexity estimate for LDA = %f' % \\\n",
    "                (iteration, bound_LDA)\n",
    "            perplexity_LDA.write(str(bound_LDA) + '\\n')\n",
    "        \n",
    "        # Save lambda, the parameters to the variational distributions\n",
    "        # over topics, and gamma, the parameters to the variational\n",
    "        # distributions over topic weights for the articles analyzed in\n",
    "        # the last iteration.\n",
    "        #if (iteration % 10 == 0):\n",
    "        #    numpy.savetxt('lambda-%d.dat' % iteration, olda._lambda)\n",
    "        #    numpy.savetxt('gamma-%d.dat' % iteration, gamma)\n",
    "\n",
    "        \n",
    "f()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import sys, re, time, string\n",
    "import numpy as np\n",
    "from scipy.special import gammaln, psi\n",
    "\n",
    "import corpus\n",
    "import numpy as np\n",
    "from scipy.misc import logsumexp\n",
    "np.random.seed(100000001)\n",
    "meanchangethresh = 0.001\n",
    "def log_beta(v):\n",
    "    \"\"\"\n",
    "    For a vector v, this function computes the log of the multivariate beta function\n",
    "    evaluated at v.\n",
    "    \"\"\"\n",
    "    return(sum(gammaln(v))-gammaln(sum(v)))\n",
    "def dirichlet_expectation(alpha):\n",
    "    \"\"\"\n",
    "    If alpha is a vector, then computes E[log(theta)] \n",
    "    given alpha for a vector theta ~ Dir(alpha).\n",
    "    If the input alpha is a matrix, then compute E[log(theta)] \n",
    "    given each row of alpha for a vector theta ~ Dir(alpha).\n",
    "    \"\"\"\n",
    "    if (len(alpha.shape) == 1):\n",
    "        return(psi(alpha) - psi(np.sum(alpha)))\n",
    "    return(psi(alpha) - psi(np.sum(alpha, 1))[:, np.newaxis])\n",
    "\n",
    "def parse_doc_list(docs, vocab):\n",
    "    \"\"\"\n",
    "    Parse a document into a list of word ids and a list of counts,\n",
    "    or parse a set of documents into two lists of lists of word ids\n",
    "    and counts.\n",
    "\n",
    "    Arguments: \n",
    "    docs:  List of D documents. Each document must be represented as\n",
    "           a single string. (Word order is unimportant.) Any\n",
    "           words not in the vocabulary will be ignored.\n",
    "    vocab: Dictionary mapping from words to integer ids.\n",
    "\n",
    "    Returns a pair of lists of lists. \n",
    "\n",
    "    The first, wordids, says what vocabulary tokens are present in\n",
    "    each document. wordids[i][j] gives the jth unique token present in\n",
    "    document i. (Don't count on these tokens being in any particular\n",
    "    order.)\n",
    "\n",
    "    The second, wordcts, says how many times each vocabulary token is\n",
    "    present. wordcts[i][j] is the number of times that the token given\n",
    "    by wordids[i][j] appears in document i.\n",
    "    \"\"\"\n",
    "    if (type(docs).__name__ == 'str'):\n",
    "        temp = list()\n",
    "        temp.append(docs)\n",
    "        docs = temp\n",
    "\n",
    "    D = len(docs)\n",
    "    \n",
    "    wordids = list()\n",
    "    wordcts = list()\n",
    "    for d in range(0, D):\n",
    "        docs[d] = docs[d].lower()\n",
    "        docs[d] = re.sub(r'-', ' ', docs[d])\n",
    "        docs[d] = re.sub(r'[^a-z ]', '', docs[d])\n",
    "        docs[d] = re.sub(r' +', ' ', docs[d])\n",
    "        words = string.split(docs[d])\n",
    "        ddict = dict()\n",
    "        for word in words:\n",
    "            if (word in vocab):\n",
    "                wordtoken = vocab[word]\n",
    "                if (not wordtoken in ddict):\n",
    "                    ddict[wordtoken] = 0\n",
    "                ddict[wordtoken] += 1\n",
    "        wordids.append(ddict.keys())\n",
    "        wordcts.append(ddict.values())\n",
    "\n",
    "    return((wordids, wordcts))\n",
    "\n",
    "class OnlineLDA:\n",
    "    \"\"\"\n",
    "    Implements online VB for Multi-layer LDA as described in (Chen 2016).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, vocab, K1, K2, D, eta0,eta1,eta2, tau0, kappa):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "        K1,K2: The size of the first and second hidden layers.\n",
    "        vocab: A set of words to recognize. Ignore the words not in this set.\n",
    "        D: Total number of documents in the population, or an estimate \n",
    "        of the number in the truely online setting.\n",
    "        eta0,eta1,eta2: Hyperparameters for the weight matrices in the first, second and third layers respectively.  \n",
    "        tau0: A (positive) learning parameter that downweights early iterations\n",
    "        kappa: Learning rate: exponential decay rate---should be between\n",
    "             (0.5, 1.0] to guarantee asymptotic convergence.\n",
    "\n",
    "        Note that if you pass the same set of D documents in every time and\n",
    "        set kappa=0 this class can also be used to do batch VB.\n",
    "        \"\"\"\n",
    "\n",
    "        self._K1 = K1\n",
    "        self._K2 = K2\n",
    "        self._W = len(vocab)\n",
    "        self._D = D\n",
    "        self._eta0 = eta0\n",
    "        self._eta1 = eta1\n",
    "        self._eta2 = eta2\n",
    "        self._const = K1 * (self._W * gammaln(eta0) - gammaln(self._W * eta0)) + K2 * (K1 *gammaln(eta1) - gammaln(K1 * eta1))\n",
    "        self._tau0 = tau0 + 1\n",
    "        self._kappa = kappa\n",
    "        self._updatect = 0\n",
    "        # Initialize the variational distribution q(W|lambda).\n",
    "        self._lambda0 = np.random.gamma(100., 1./100., (self._K1, self._W))\n",
    "        self._ElogW0 = dirichlet_expectation(self._lambda0)\n",
    "        self._lambda1 = np.random.gamma(100., 1./100., (self._K2, self._K1))        \n",
    "        self._ElogW1 = dirichlet_expectation(self._lambda1)\n",
    "        with open('dataset/wordids-test.p', 'rb') as f:\n",
    "            self._wordids_test = cPickle.load(f)\n",
    "        with open('dataset/wordcts-test.p', 'rb') as f:\n",
    "            self._wordcts_test = cPickle.load(f)\n",
    "    def do_e_step(self, wordids, wordcts):\n",
    "        \"\"\"\n",
    "        Arguments: wordids and wordcts are two lists. Each element in \n",
    "        lists are from a separate incoming documents. \n",
    "        This function updates the local variational parameters phi and \n",
    "        gamma, with fixed lambda.\n",
    "        \"\"\"\n",
    "        batchD = len(wordids)\n",
    "        # Initialize the variational distribution q(theta|gamma) for\n",
    "        # the mini-batch.\n",
    "        #gamma = np.random.gamma(100., 1./100., (batchD, self._K2))\n",
    "        #ElogW2 = dirichlet_expectation(gamma)\n",
    "        #expElogW2 = np.exp(ElogW2)\n",
    "        \n",
    "        ## Initialize the variational distribution q(z|phi) for the mini-batch.\n",
    "        \n",
    "        meanchange = 0\n",
    "        def update_gammaphi(d):\n",
    "            ids = wordids[d]\n",
    "            cts = wordcts[d]\n",
    "            Wd = len(ids)\n",
    "            ElogW0d = self._ElogW0[:,ids]\n",
    "            phi1 = np.tile(1/self._K1,(len(ids),self._K1))\n",
    "            phi2 = np.tile(1/self._K2,(len(ids),self._K2))\n",
    "            gamma = np.random.gamma(100., 1./100.,self._K2)\n",
    "            ElogW2 = dirichlet_expectation(gamma)\n",
    "            for it in range(100):\n",
    "                lastgamma = gamma\n",
    "                # Wd * K1 matrix\n",
    "                unnormalized_log_phi1 = np.dot(phi2,self._ElogW1) + np.transpose(ElogW0d)\n",
    "                # Wd vector\n",
    "                phi1normalizer = map(logsumexp,unnormalized_log_phi1)\n",
    "                # Wd * K1 matrix\n",
    "                phi1 = np.exp(unnormalized_log_phi1 - np.transpose(np.tile(phi1normalizer,(self._K1,1))))\n",
    "                # Wd * K2 matrix\n",
    "                unnormalized_log_phi2 = np.dot(phi1,np.transpose(self._ElogW1)) + np.tile(ElogW2,(Wd,1)) \n",
    "                # Wd vector\n",
    "                phi2normalizer = map(logsumexp,unnormalized_log_phi2)\n",
    "                # Wd * K2 matrix\n",
    "                phi2 = np.exp(unnormalized_log_phi2 - np.transpose(np.tile(phi2normalizer,(self._K2,1))))\n",
    "                # K2 vector\n",
    "                gamma = np.tile(self._eta2,self._K2) + np.dot(np.transpose(phi2),cts)\n",
    "                meanchange = np.mean(abs(gamma - lastgamma))\n",
    "                if (meanchange < meanchangethresh):\n",
    "                    break\n",
    "            # Compute phi1 times cts \n",
    "            # Psi1: K1 * Wd\n",
    "            # Psi2: K2 * K1\n",
    "            Psi1 = np.multiply(np.transpose(phi1),cts)\n",
    "            Psi2 = np.transpose(np.dot(Psi1,phi2))\n",
    "            return((ids,cts,Psi1,Psi2,gamma,phi1,phi2))\n",
    "  \n",
    "        return(map(update_gammaphi,range(0,batchD)))\n",
    "    \n",
    "    def update_lambda_docs(self, wordids, wordcts):\n",
    "        rhot = pow(self._tau0 + self._updatect, -self._kappa)\n",
    "        self._rhot = rhot\n",
    "        gamma_phi = self.do_e_step(wordids, wordcts)\n",
    "        \n",
    "        for d in range(len(gamma_phi)):\n",
    "            # self._lambda0[:,gamma_phi[d][0]]: K1 * Wd.\n",
    "            self._lambda0[:,gamma_phi[d][0]] = self._lambda0[:,gamma_phi[d][0]] * \\\n",
    "            (1-rhot) + rhot * (np.tile(self._eta0,(self._K1,len(gamma_phi[d][0]))) + \\\n",
    "                               self._D * gamma_phi[d][2] / np.float(len(wordids)))\n",
    "            # self._lambda1: K2 * K1\n",
    "            self._lambda1 = self._lambda1 * (1-rhot) + rhot * (np.tile(self._eta1,(self._K2,self._K1)) \\\n",
    "                       + self._D * gamma_phi[d][3] / np.float(len(wordids)))\n",
    "        \n",
    "        self._ElogW0 = dirichlet_expectation(self._lambda0)       \n",
    "        self._ElogW1 = dirichlet_expectation(self._lambda1)\n",
    "         \n",
    "                                 \n",
    "    def perplexity_approx(self,gamma_phi):\n",
    "        score0 = sum(log_beta(self._lambda0)) + sum(log_beta(self._lambda1))  \n",
    "        score0 += np.sum(np.multiply(np.tile(self._eta0,(self._K1,self._W)) - self._lambda0,self._ElogW0))\n",
    "        score0 += np.sum(np.multiply(np.tile(self._eta1,(self._K2,self._K1)) - self._lambda1,self._ElogW1))\n",
    "        ratio = self._D / np.float(len(gamma_phi))\n",
    "        # This function approximates the perplexity before updating the \n",
    "        # global parameters for a particular document.\n",
    "        def perplexityd(d):\n",
    "            (ids,cts,Psi1,Psi2,gamma,phi1,phi2) = gamma_phi[d]\n",
    "            score = np.sum(np.multiply(self._ElogW0[:,ids],Psi1))\n",
    "            score += np.sum(np.multiply(Psi2,self._ElogW1))\n",
    "            score += np.dot(dirichlet_expectation(gamma),np.dot(np.transpose(phi2),cts) + \\\n",
    "                             np.tile(self._eta2,(self._K2)) - gamma)\n",
    "            score += (log_beta(gamma) - log_beta(np.tile(self._eta2,(self._K2))))\n",
    "            score = score - np.sum(np.dot(np.transpose(np.multiply(phi1,np.log(phi1))),cts))\n",
    "            score = score - np.sum(np.dot(np.transpose(np.multiply(phi2,np.log(phi2))),cts))\n",
    "            return(score)\n",
    "\n",
    "        totalscore = ratio * sum(map(perplexityd,range(len(gamma_phi)))) - self._const + score0\n",
    "        numtotalwords = sum([sum(gamma_phi[d][1]) for d in range(len(gamma_phi))])                           \n",
    "        return(np.exp(- totalscore / np.float(numtotalwords * ratio)))                           \n",
    "     \n",
    "    def perplexity_for_test(self):\n",
    "        gamma_phi_test = self.do_e_step(self._wordids_test, self._wordcts_test)\n",
    "        bound = self.perplexity_approx(gamma_phi_test)\n",
    "        return(bound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: held-out perplexity estimate = 12316.713138\n",
      "1: held-out perplexity estimate = 4563.764539\n"
     ]
    }
   ],
   "source": [
    "import cPickle, string, numpy, getopt, sys, random, time, re, pprint\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import twolayerfunctions\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "from multiprocessing import Pool\n",
    "    \n",
    " \n",
    "\"\"\"\n",
    "This function fits Wikipedia articles to the deep LDA model in an online style.\n",
    "\"\"\"\n",
    "\n",
    "# The number of documents to analyze in each iteration\n",
    "batchsize = 32\n",
    "# The estimated total number of documents  \n",
    "D = 5.13e6\n",
    "# The number of topics\n",
    "K1 = 50\n",
    "K2 = 10\n",
    "eta0 = 1 / np.float(K1)\n",
    "eta1 = 1 / np.float(K2)\n",
    "eta2 = 1 / np.float(K2) \n",
    "vocab = file('dataset/vocab.nytimes.txt').readlines()\n",
    "# The total number of iterations\n",
    "M = 2\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "def fit_model_revised(parameter):\n",
    "    return(twolayerfunctions.fit_model(parameter,M,D))\n",
    "\n",
    "vocab = file('dataset/vocab.nytimes.txt').readlines()\n",
    "W = len(vocab)\n",
    "\n",
    "# Initialize the algorithm.\n",
    "# Create a grid of K1,K2,rho. \n",
    "# Create a list of tuples to store parameters.\n",
    "a =  [[[(K1,K2,rho[1],rho[0]) for rho in [(0.7,256),(0.7,1024),(0.8,512),(0.9,256),(1,0)]] for K1 in [1,10,30,50,100]] \n",
    "          for K2 in [1,10,30,50,100]] \n",
    "b = [item for sublist in a for item in sublist]\n",
    "parameters = [item for sublist in b for item in sublist]\n",
    "#pool = multiprocessing.Pool(num_cores)\n",
    "#pool.map(fit_model_revised , parameters)   \n",
    "twolayerfunctions.fit_model(parameters[0] ,M,vocab,D,eta0, eta1, eta2)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create files and python jobs so as to run my codes on Linux Cluster.\n",
    "i = 0\n",
    "for K1 in [1,10,30,50,100]:\n",
    "    for K2 in [1,10,30,50,100]:\n",
    "        for rho1 in [256,1024]:\n",
    "            for rho2 in [0.7,0.9,1]:\n",
    "                i = i + 1\n",
    "                with open('ny-%d.py' %i, 'w') as f:\n",
    "                    f.write(\"import nytimes_analysis\"+\"\\n\"+\"nytimes_analysis.f(%d,%d,%d,%f)\" %(K1,K2,rho1,rho2))\n",
    "                with open('jobs-%d.sh' %i,'w') as f:\n",
    "                    f.write(\"#python\\n\"+\"python ny-%d.py\" %i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
